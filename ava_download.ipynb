{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import logging\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    datefmt='%m-%d %H:%M')\n",
    "\n",
    "request_headers = {'User-Agent' : 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'}\n",
    "\n",
    "class WebParser(object):\n",
    "    def __init__(self, wait_second=1, max_retry_time=10):\n",
    "        self.html_cache_path = \"./data/html_cache/\"\n",
    "        self.img_path = \"./data/img_cache/\"\n",
    "        self.soup = None\n",
    "        self.wait_second = wait_second\n",
    "        self.max_retry_time = max_retry_time\n",
    "    \n",
    "    def build_request_url(self, imgid):\n",
    "        url_pattern = \"http://www.dpchallenge.com/image.php?IMAGE_ID=%s\"\n",
    "        return url_pattern%imgid\n",
    "    \n",
    "    def build_request_headers(self):\n",
    "        user_agent = \"Mozilla/5.0 (X11; U; Linux x86_64; en-US; rv:1.9.2.24) Gecko/20111109 CentOS/3.6.24-3.el6.centos Firefox/3.6.24\"\n",
    "        request_headers = {'User-Agent': user_agent}\n",
    "        return request_headers\n",
    "\n",
    "    def load_html(self, imgid):\n",
    "        cache_file = self.html_cache_path + \"%s.html\"%imgid\n",
    "        if os.path.exists(cache_file):\n",
    "            logging.info(\"[imgid=%s]html has been cached.\"%imgid)\n",
    "            html = open(cache_file, 'r').read()\n",
    "        else:\n",
    "            url = self.build_request_url(imgid)\n",
    "            headers = self.build_request_headers()\n",
    "            is_opened = False\n",
    "            for _ in range(self.max_retry_time):\n",
    "                try:\n",
    "                    html = requests.get(url=url, headers=headers).content\n",
    "                else:\n",
    "                    self.save_html(html, cache_file)\n",
    "                    logging.info(\"[imgid=%s]download html successfully.\"%imgid)\n",
    "                    is_opened = True\n",
    "                    break\n",
    "                finally:\n",
    "                    time.sleep(self.wait_second)\n",
    "            if not is_opened:\n",
    "                logging.warning(\"[imgid=%s]download html failed.\"%imgid)\n",
    "                return False\n",
    "        self.soup = BeautifulSoup(html, 'html.parser')\n",
    "        return True\n",
    "\n",
    "    def save_html(self, html, cache_file):\n",
    "        fhtml = open(cache_file, 'w')\n",
    "        fhtml.write(html)\n",
    "        fhtml.close()\n",
    "\n",
    "    def save_image(self, imgid):\n",
    "        cached_img = self.img_path + \"%s.jpg\"%imgid\n",
    "        if os.path.exists(cached_img):\n",
    "            logging.info(\"[imgid=%s]image has been cached.\"%imgid)\n",
    "        else:\n",
    "            img_url = self.get_img_url()\n",
    "            if img_url is None:\n",
    "                logging.warning(\"[imgid=%s]image does not exist.\"%imgid)\n",
    "                return False\n",
    "            try:\n",
    "                urllib.urlretrieve(img_url, cached_img)\n",
    "            except Exception, e:\n",
    "                logging.warning(\"[imgid=%s]image caches failed.\"%imgid)\n",
    "                return False\n",
    "            else:\n",
    "                logging.info(\"[imgid=%s]image caches successfully.\"%imgid)\n",
    "        return True\n",
    "\n",
    "    def get_img_url(self):\n",
    "        img_container = self.soup.find(\"td\", id=\"img_container\")\n",
    "        if img_container is None or len(img_container.contents) < 2:\n",
    "            return None\n",
    "        else:\n",
    "            return img_container.contents[1].get(\"src\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "web_parser = WebParser(wait_second=1, max_retry_time=10)\n",
    "\n",
    "with open(\"./data/ava/AVA.txt\", 'r') as fin:\n",
    "    for i, line in enumerate(fin):\n",
    "        fields = line.strip().split(\" \")\n",
    "        imgid = fields[1]\n",
    "        print imgid\n",
    "        web_parser.load_html(imgid)\n",
    "        web_parser.save_image(imgid)\n",
    "        if i == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
